# cli.py ‚Äî Edge-TTS + Wav2Lip –ø–∞–∫–µ—Ç —Ä–µ–∂–∏–º—ñ (“õ–æ—Å JSONL –ª–æ–≥)
# --------------------------------------------------------------------
# ‚ë† “∞–∑—ã–Ω –º”ô—Ç—ñ–Ω–¥—ñ –µ–Ω–≥—ñ–∑—ñ“£—ñ–∑ ‚Üí –±–æ—Å –∂–æ–ª (Enter) ‚Üí –∞—è“õ—Ç–∞—É
# ‚ë° –ê–≤–∞—Ç–∞—Ä –∂—ã–Ω—ã—Å—ã–Ω —Ç–∞“£–¥–∞“£—ã–∑ (m ‚Äì –µ—Ä, f ‚Äì ”ô–π–µ–ª)
# ‚ë¢ ”ò—Ä —Å–µ–≥–º–µ–Ω—Ç–∫–µ –±—ñ—Ä–µ–≥–µ–π action (1-20) —Ç–∞“ì–∞–π—ã–Ω–¥–∞—É, –∞–ª–¥—ã–Ω-–∞–ª–∞ –∫–µ—Å—Ç–µ –∫”©—Ä—Å–µ—Ç—É
# ‚ë£ MAX_WORKERS (3) –∞“ì—ã–Ω–º–µ–Ω:
#      ‚Ä¢ Edge-TTS (16 kHz WAV)
#      ‚Ä¢ Wav2Lip (–∞—É—ã–∑ “õ–∏–º—ã–ª—ã) ‚Äî stdout –ø—Ä–æ–≥—Ä–µ—Å—Å—ñ –ø—Ä–µ—Ñ–∏–∫—Å–ø–µ–Ω
#      ‚Ä¢ clip –±—ñ—Ç–∫–µ–Ω–¥–µ OutputLogger-–≥–µ –∂–∞–∑—É
# ‚ë§ static/logs/ —ñ—à—ñ–Ω–¥–µ –µ–∫—ñ —Ñ–∞–π–ª:
#      session_*_ids.jsonl     ‚Äî API –¥–µ“£–≥–µ–π—ñ–Ω–¥–µ–≥—ñ ID-–ª–µ—Ä
#      session_*_clips.jsonl   ‚Äî –¥–∞–π—ã–Ω –∫–ª–∏–ø –∞“õ–ø–∞—Ä–∞—Ç—ã
# ‚ë• –ö–ª–∏–ø—Ç–µ—Ä –∂–æ–π—ã–ª–º–∞–π–¥—ã; “õ–∞–ª–∞—Å–∞“£—ã–∑ ffmpeg –±—ñ—Ä—ñ–∫—Ç—ñ—Ä—É “±—Å—ã–Ω—ã–ª–∞–¥—ã
# --------------------------------------------------------------------

import os, sys, subprocess, pathlib, concurrent.futures
from typing import List, Optional

from utils.nlp import parse_text
from utils.tts import synthesize_speech
from utils.video_utils import TEMPLATE_DIR, OUTPUT_DIR, WAV2LIP_DIR
from utils.merge import concat_videos
from utils.classify import classify_sentence_structure
from utils.api_id import IDLogger
from utils.output_id import OutputLogger

AUDIO_DIR = pathlib.Path("static/audio").resolve(); AUDIO_DIR.mkdir(exist_ok=True)
LOG_DIR   = pathlib.Path("static/logs").resolve();  LOG_DIR.mkdir(exist_ok=True)
MAX_WORKERS = 3

# ------------------------------------------------------------------ helpers
def assign_actions(sentences: List[str], gender: str):
    """[(idx, sentence, action_id, template)]"""
    mapping = []
    for idx, sent in enumerate(sentences, 1):
        action_id, _ = classify_sentence_structure(None)
        mapping.append((idx, sent, action_id, f"{gender}_{action_id}.mp4"))
    return mapping


def run_clip(idx: int, sentence: str, gender: str, action_id: int,
             clip_logger: OutputLogger) -> str:
    """TTS + Wav2Lip for single clip; write clip log when ready."""
    # ---------- 1) Edge-TTS
    audio_path = AUDIO_DIR / f"seg_{idx:03d}.wav"
    synthesize_speech(sentence, str(audio_path), voice_gender=gender)

    # ---------- 2) Wav2Lip
    template  = TEMPLATE_DIR / f"{gender}_{action_id}.mp4"
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    out_path  = OUTPUT_DIR / f"vid_{idx:03d}.mp4"

    cmd = [
        "python", str(WAV2LIP_DIR / "inference.py"),
        "--checkpoint_path", str(WAV2LIP_DIR / "checkpoints/wav2lip_gan.pth"),
        "--segmentation_path", str(WAV2LIP_DIR / "checkpoints/face_segmentation.pth"),
        "--enhance_face", "gfpgan",
        "--face", str(template),
        "--audio", str(audio_path),
        "--outfile", str(out_path),
    ]
    print(f"[{idx:02d}] üéûÔ∏è  Lip Sync –±–∞—Å—Ç–∞–ª–¥—ã (action {action_id})")
    proc = subprocess.Popen(cmd, cwd=str(WAV2LIP_DIR),
                            stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                            text=True, env={**os.environ, "CUDA_VISIBLE_DEVICES": "0"})
    assert proc.stdout
    for line in proc.stdout:
        print(f"[{idx:02d}] {line.rstrip()}")
    proc.wait()
    if proc.returncode != 0:
        raise RuntimeError(f"Wav2Lip {idx} –∫–ª–∏–ø—ñ “õ–∞—Ç–µ ({proc.returncode})")

    print(f"[{idx:02d}] ‚úì –î–∞–π—ã–Ω ‚Üí {out_path}")

    # ---------- 3) Clip-level log
    clip_logger.add_entry(
        text_clip_id   = idx,
        video_path     = str(out_path),
        avatar_action_id = action_id
    )
    return str(out_path)

# ------------------------------------------------------------------ main
def main():
    # 1. –ú”ô—Ç—ñ–Ω –µ–Ω–≥—ñ–∑—É (–±–æ—Å –∂–æ–ª ‚Äì —Ç–æ“õ—Ç–∞—Ç—É)
    print("“∞–∑—ã–Ω –º”ô—Ç—ñ–Ω–¥—ñ –µ–Ω–≥—ñ–∑—ñ“£—ñ–∑ (–∞—è“õ—Ç–∞—É “Ø—à—ñ–Ω –±–æ—Å –∂–æ–ª–¥–∞ Enter –±–∞—Å—ã“£—ã–∑):")
    lines: List[str] = []
    while True:
        ln = input()
        if ln.strip() == "":
            break
        lines.append(ln)
    text = "\n".join(lines)

    # 2. –°–µ–≥–º–µ–Ω—Ç—Ç–µ—É
    sentences, _ = parse_text(text)

    # 3. –ê–≤–∞—Ç–∞—Ä –∂—ã–Ω—ã—Å—ã–Ω —Ç–∞“£–¥–∞—É
    while True:
        gender = input("–ê–≤–∞—Ç–∞—Ä –∂—ã–Ω—ã—Å—ã (m ‚Äì –µ—Ä, f ‚Äì ”ô–π–µ–ª): ").strip().lower()
        if gender in {"m", "f"}:
            break

    # 4. Action —Ç–∞“ì–∞–π—ã–Ω–¥–∞—É –∂”ô–Ω–µ –∫–µ—Å—Ç–µ
    mapping = assign_actions(sentences, gender)
    print("\n=== –ö–ª–∏–ø ‚Äì –®–∞–±–ª–æ–Ω —Å”ô–π–∫–µ—Å—Ç—ñ–≥—ñ ===")
    for idx, sent, aid, tpl in mapping:
        print(f"{idx:02d}: {sent}  ‚Üí  {tpl}")
    input("\n–ë–∞—Å—Ç–∞—É “Ø—à—ñ–Ω Enter –±–∞—Å—ã“£—ã–∑‚Ä¶")

    # 5. –õ–æ–≥–≥–µ—Ä–ª–µ—Ä
    api_logger   = IDLogger(LOG_DIR)
    clip_logger  = OutputLogger(LOG_DIR)
    avatar_gender_id = 1 if gender == "m" else 2
    voice_gender_id  = avatar_gender_id

    #   5-a. –ê–ª–¥—ã–Ω-–∞–ª–∞ API-ID –∂–∞–∑—É
    for idx, _s, aid, _tpl in mapping:
        api_logger.add_entry(
            text_clip_id      = idx,
            orig_voice_id     = 1000 + idx,
            avatar_action_id  = aid,
            avatar_gender_id  = avatar_gender_id,
            voice_gender_id   = voice_gender_id,
            target_voice_id   = None,
            after_voice_id    = None,
        )

    # 6. –ö–ª–∏–ø—Ç–µ—Ä–¥—ñ –ø–∞—Ä–∞–ª–ª–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—è–ª–∞—É
    videos: List[Optional[str]] = [None] * len(mapping)
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:
        fut_map = {
            pool.submit(run_clip, idx, sent, gender, aid, clip_logger): idx
            for idx, sent, aid, _tpl in mapping
        }
        for fut in concurrent.futures.as_completed(fut_map):
            idx = fut_map[fut]
            try:
                videos[idx-1] = fut.result()
            except Exception as exc:
                print(f"[{idx:02d}] ‚úó “ö–∞—Ç–µ: {exc}")

    # 7. –õ–æ–≥ —Ñ–∞–π–ª–¥–∞—Ä—ã
    print(f"\nAPI ID –ª–æ–≥—ã  ‚Üí {api_logger.file_path()}")
    print(f"–ö–ª–∏–ø –ª–æ–≥—ã    ‚Üí {clip_logger.file_path()}")

    # 8. –ë—ñ—Ä—ñ–∫—Ç—ñ—Ä—É —Å“±—Ä–∞—É
    if input("\n–ë–∞—Ä–ª—ã“õ –∫–ª–∏–ø—Ç–µ—Ä–¥—ñ –±—ñ—Ä –±–µ–π–Ω–µ–≥–µ –±—ñ—Ä—ñ–∫—Ç—ñ—Ä—É–¥—ñ “õ–∞–ª–∞–π—Å—ã–∑ –±–∞? (y/n): ").lower() == "y":
        ready = [v for v in videos if v]
        if not ready:
            print("–ë—ñ—Ä—ñ–∫—Ç—ñ—Ä—É–≥–µ –∂–∞—Ä–∞–º–¥—ã –∫–ª–∏–ø –∂–æ“õ!")
            return
        final_mp4 = "static/video_output/final_merged.mp4"
        print("üîó ffmpeg –∞—Ä“õ—ã–ª—ã –±—ñ—Ä—ñ–∫—Ç—ñ—Ä—É ‚Ä¶")
        concat_videos(ready, final_mp4)
        print(f"üé¨ –î–∞–π—ã–Ω –±–µ–π–Ω–µ ‚Üí {final_mp4}")
    else:
        print("–ñ–µ–∫–µ –∫–ª–∏–ø—Ç–µ—Ä static/video_output/ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Å—ã–Ω–¥–∞ —Å–∞“õ—Ç–∞–ª–¥—ã.")


if __name__ == "__main__":
    main()
